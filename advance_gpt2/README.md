# 🤖 GPT-2 Text Generation – ShadowFox AIML Internship (Advanced Task)

## 📌 Project Overview
This project is part of the **Advanced Level Task** for the *ShadowFox AIML Internship*.  
The objective is to explore **Language Models (LMs)** and demonstrate the capability of **GPT-2** for **text generation**.

GPT-2 is a transformer-based model trained on large-scale internet text.  
Here, we experiment with **prompt-based generation**, analyze its strengths and limitations, and demonstrate potential applications.

---

## 🎯 Objectives
- Load a pre-trained **GPT-2 model** using Hugging Face Transformers.
- Provide custom **prompts** and generate continuation text.
- Experiment with **sampling parameters** like *temperature*, *top-p*, and *max_length*.
- Evaluate the generated outputs qualitatively.
- Summarize **insights, strengths, and limitations** of GPT-2.

---

## ⚙ Tech Stack
- **Language**: Python 🐍
- **Libraries**:
  - `transformers` (Hugging Face 🤗)
  - `torch`
  - `numpy`
  - `matplotlib` (optional, for visualization)

---

## 🔑 Steps Followed
1. Imported GPT-2 tokenizer and model from Hugging Face.
2. Set a **prompt** for the model to complete.
3. Generated text using **sampling** (temperature, top-p).
4. Compared outputs with different parameters.
5. Analyzed results and summarized findings.

---

## 📊 Example Output

**Prompt:**  
